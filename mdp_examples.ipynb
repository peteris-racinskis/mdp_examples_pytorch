{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent models and decision processes: key concepts with examples - reinforcement and imitation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the field of artificial intelligence, when one wishes to create a system that observes the environment and performs some kind of action in response, one generally thinks of this as a **decision process**. This is of particular interest to researchers seeking to employ machine learning methods in robotics. When reading discourse surrounding this broad topic, it is quite common to come across terms such as \"agents\", \"policies\", \"states\", \"actions\", \"reinforcement learning\", \"reward functions\" and so forth. This notebook is intended to serve as a gentle introduction to these concepts and the more common theoretical frameworks associated with them. It also includes some simple worked examples of learning a policy for balancing a pole on a cart using reinforcement and imitation learning.\n",
    "\n",
    "Bear in mind that is an extremely brief introduction to a *huge* field of active research, so there are many things not covered here, even basic ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly and informally, a **decision process** is a process where an **agent** takes an **observation** of the **state** of its **environment** and comes up with an **action** it should take using a **policy**. Now, that's a lot of big words all at once, so in more detail about each:\n",
    "\n",
    "- **environment** -- a real or simulated \"world\" where objects exist in some configuration (its **state**). For the purposes of decision processes, it is usually assumed that this world has time which progresses in discrete steps;\n",
    "- **state** -- a vector fully describing the configuration of everything that exists in the environment. This changes with time, and since time progresses in discrete steps, any environment can be described as a sequence of its states as they evolve over time $ (s_1, s_{2}, ... ) $. The state belongs to a set of all possible states $ s \\in S $. The variables that make up this **state** vector can be discrete or continuous;\n",
    "- **observation** -- a vector that includes some subset of the **state** variables. This is necessary because in most cases, the complete **state** of the system is impractical or impossible to observe. If the **observation** gives us enough information to fully determine the future **state** of everything we care about, it is generally treated the same as if it were the complete **state**;\n",
    "- **agent** -- an entity, such as a robot controller, animal, human or AI subroutine managing a character in a video game, that exists in some sort of **environment**. What sets an agent apart from just any old object is that it can **observe** the **state** of its **environment** and possesses the means to alter it in some way (such as a means of locomotion to alter its own position, or actuators to move other things);\n",
    "- **action** -- something the **agent** can do to influence the **state** of the environment. The **action** belongs to a set of all possible actions $ a \\in A $ This is typically paired with a **transition function** which describes how the future state of the environment is altered when a given **action** is taken at a given **state**. The **transition function** can be deterministic $ T(s_t, a) = s_{t+1}$ or probabilistic $ T(s_t, s_{t+1}, a) = P(s_{t+1} \\vert s_t, a) $. The variables that make up the **action** vector can be discrete or continuous, but in most classic reinforcement and imitation learning contexts we only deal with discrete actions;\n",
    "- **policy** -- a function which dictates how the agent will respond to any given state in its environment. \n",
    "\n",
    "Now that we're familiar with the terms, we can turn to defining a specific **decision process**. There are many ways one could theoretically go about doing this, but in practice almost every paper related to reinforcement and imitation learning (until some very recent developments, see below) starts by laying down the definition of a **Markov Decision Process**, or MDP for short. There is some variability in how people choose to state it, but usually it will be something like\n",
    "\n",
    "$$ MDP = (S, A, R, T) $$\n",
    "\n",
    "where $ S $ represents the set of all possible states, $ A $ is the set of all possible actions, $ R: S \\rightarrow \\mathbb{R} $ is the **reward** function which tells us how well we did by ending up in this state and $ T: S \\times A \\rightarrow S \\text{ or } P(S) $ is the (deterministic or probabilistic) transition function. There's quite a bit of wiggle room over the specifics of stating this (introducing objects such as $ \\Gamma : S \\rightarrow A $, a function that tells us what actions are available at which states) but the main takeaway here is that **the future state of the system depends only on the current state and the action taken by the agent**. This means there is no hysteresis -- we don't care about anything that happened before the current state or how we got here. Thus most of the theory dealing with this concept also assumes that any given agent will define the **policy** as $ \\pi : S \\rightarrow A \\text{ or } P(A) $. \n",
    "\n",
    "The primary question the fields of **reinforcement and imitation learning** seek to answer is thus: *given a Markov Decision Process, what is the most effective way of discovering a policy function?* Both are discussed in more detail below, but the long and short of it is:\n",
    "\n",
    "- In **reinforcement learning** we give our agent a random starting policy (typically in the form of some model template, an object which can approximate a wide range of possible functions -- such as a neural network) and let it loose upon the environment. This way we collect rewards which tell us how well we did. Using the rewards, we update the policy, then try again. Repeat until a useful agent pops out. The biggest advantage of this approach is that we don't have to know anything about how to actually achieve the goal, nor do we need a system that can. In theory, we only need to be able to define the reward function. In practice, however, there are several important drawbacks. First of all, this method is computationally extremely expensive, since the only way to learn is through interaction with the (typically simulated) environment. Secondly, many problems have extremely sparse rewards (meaning we can go a long time before seeing how well we did), which results in astronomically huge search spaces that agents have to explore before getting any useful feedback. Thus, the approach of\n",
    "- **imtation learning** takes example state-action pairs produced by an **expert** and trains a policy that can reproduce these accurately. Obviously, this only works if we have an expert (such as a human demonstrator) to begin with. But provided we do, this lets us skip the incredibly expensive exploration phase of the process and go straight to learning. Furthermore, a policy obtained from a not very good expert can then be improved through reinforcement learning, and lots of research in the field is about precisely this -- using imitation learning to \"kickstart\" a reinforcement learning process.\n",
    "\n",
    "So far, we've only discussed cases when the full state of the system is known and it follows the Markov Decision Process formalism. Things get somewhat murkier once we start dealing with incomplete observations -- it is possible that some hidden variables are not apparent from any given observation but can in fact be inferred from longer state histories. This is particularly often the case when dealing with agents which will have to operate in real rather than simulated environments -- such as robot controllers. Therefore the most recent state-of-the-art research, particularly regarding imitation learning, has been quite heavily slanted towards using sequence prediction models (such as transformer neural networks) which also keep track of past states, moving us away from the world of MDPs. However, in many cases methods grounded in the MDP are perfectly sufficient and those are the only ones used in examples below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The task - CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task used in all the examples below is a classic control problem called the **inverse pendulum**. Basically, you've got a stick on a cart. The stick is free to rotate, the cart is free to move left and right. The agent's got to keep the stick upright by sliding the cart underneath. If the stick goes below a certain angle, or the cart slides out of the picture, game over. For every timestep that the agent manages to survive it gets a $+1$ reward. This is great introductory illustration of various control-related concepts. In our case, we're using the *OpenAI gym* package to provide us with a ready-made simulation environment that accepts actions as inputs and outputs states at every timestep. Each action is a number -- either 0 or 1 -- that tells the cart to move left or right. Each state is a vector $ (x, v, \\theta, \\omega) $ where $x$ -- current position, $v$ -- current velocity, $\\theta$ -- angle of the stick, $\\omega$ -- rotational velocity of the stick. Visualization is handled by the *gym* package. All that's left for us to do is to write our own agent.\n",
    "\n",
    "To get a picture of what this looks like with an untrained agent, below is an agent using the tried and true \"take a random action every time\" approach to success. *visualize_model* is a utility function that wraps the simulation environment for us, defined in *utils.py* in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import visualize_model\n",
    "\n",
    "class RandomAgent():\n",
    "    \n",
    "    def get_action(self, _):\n",
    "        return np.random.randint(0,2)\n",
    "\n",
    "model = RandomAgent()\n",
    "visualize_model(model, notebook=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reinforcement learning, the agent takes actions in its environment, collects a reward at every state it visits and uses this reward signal to optimize its policy. Simple enough to state, but an enormously broad problem in reality. A wide variety of different approaches have been studied, and it's hard to even begin trying to categorize them. But perhaps the most straightforward divide is that between value function and policy optimization methods.\n",
    "\n",
    "**Policy optimization** methods optimize the policy directly. There are many ways to do this, but in principle the pattern is as follows: the policy is a model template (such as a neural network); this is used to collect reward information; the model is updated directly to take actions which result in greater reward. These methods typically struggle with noisy reward signals.\n",
    "\n",
    "**Value function** methods try to estimate the value -- time-discounted sum of all future rewards -- of any given state, and use this estimate to select an action with the greatest expected return. The main reason to do things this way is that it lets us \"average out\" the noise in observed rewards. The Q-learning example below is a value function method.\n",
    "\n",
    "In practice, combined methods also exist. The advantage actor-critic example below is a combined method, where an estimate of the value function is optimized alongside the policy to help stabilize and guide the optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classic value function estimation method, in use since the 1980s. In its most basic form, we keep a tensor (many-dimensional equivalent to a matrix or vector) $ Q $ with $ q_{s,a} \\in Q $ corresponding to the expected value \n",
    "\n",
    "$$ \\mathbb{E}\\left[ R(T(s_t, a)) + \\sum_{\\tau={t+1}}^{\\inf} \\gamma^{\\tau-t} R(s_{\\tau}) \\right]: s_{\\tau} = T(s_{\\tau-1}, \\max_a \\lbrace q_{s_{\\tau-1},a} \\rbrace) $$\n",
    "\n",
    "Now, this equation may seem like quite the handful at first, and it's recursive to boot. Essentially, every cell contains the esimate of what we'd get if at state $s_t$ we took action $a$, but thereafter kept picking the action based on what we expect the maximum reward to be. The $ \\gamma $ term is the **discount factor**, some number between 0 and 1 that lets us weigh rewards far in the future as less important than ones closer in time.\n",
    "\n",
    "Of course, this assumes that we already have the Q-matrix filled out with all the values, which is a sort of chicken-and-egg problem. Thankfully, it's been proven that this kind of time-discounted value matrix will converge if we simply collect rewards and update the values at every time step. Specifically, the update rule used in the script is\n",
    "\n",
    "$$ q_{s_t,a}^{i+1} = (1-\\alpha)q_{s_t,a}^i + \\alpha \\sum_{\\tau=t}^{t_{\\max}}\\gamma^{\\tau-t} R(s_{\\tau}) $$\n",
    "\n",
    "$ s_t \\in (s_1, s_2, ..., s_{t_{\\max}}) $ is a state recorded in the training episode of length $t_{\\max}$, $i$ is the current training iteration, $\\alpha$ is a coefficient that determines how much we update the matrix with every training iteration. There is one major catch with using this method for the cartpole task -- the Q-matrix requires that our state values be discrete, or else the matrix would have to be infinitely large. Therefore, a large part of the implementation code is concerned with mapping the continuous position, angle and velocity observations to discrete values which can be used to index an array. A resolution has been picked arbitrarily to find a good compromise between performance and having a model that actually learns the task. The implementation code can be found in *QMatrix.py*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we first define a convenience function to update the learning rate with every training iteration. When starting out we can afford to make large updates, but as training progresses we don't want to keep throwing our model off an altering the induced state distribution (states actually visited by the agent when following a policy) too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def rate_schedule(ep, decay, min_rate = 0.01, initial = 1.0):\n",
    "    return max( min_rate, min( initial, math.exp(-decay*ep) ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual training loop is rather simple compared to more sophisticated models. At the end of our process, we can choose to save our model to a file by simply pickling the QMatrix object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_matrix(model, env, save=True):\n",
    "\n",
    "    for episode in range(4000):\n",
    "\n",
    "        lr = rate_schedule(episode, 0.001)\n",
    "        exploration_rate = rate_schedule(episode, 0.001)\n",
    "        discount = 0.98\n",
    "\n",
    "        done = False\n",
    "        state_new, _ = env.reset()\n",
    "        timestep = 0\n",
    "        \n",
    "        while not done and timestep < 2000:\n",
    "\n",
    "            timestep += 1\n",
    "            state_old = state_new\n",
    "            action, _ = model.randomized_action(env.action_space, state_old, exploration_rate)\n",
    "\n",
    "            state_new, reward, done, _, __ = env.step(action)\n",
    "            expected_reward = reward + model.expected_next_reward(state_new, discount)\n",
    "            model.update_reward(expected_reward, state_old, action, lr)\n",
    "\n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Training episode {episode} survived for {timestep} steps\")     \n",
    "    if save:\n",
    "        model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call the training loop we need to instantiate the matrix and environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from QMatrix import QMatrix\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "QSHAPE=(4,8,4,8) # how many discrete steps we split each continuous axis into\n",
    "LIMITS=(\n",
    "    (env.observation_space.low[0], env.observation_space.high[0]),\n",
    "    (-10,10),\n",
    "    (env.observation_space.low[2], env.observation_space.high[2]),\n",
    "    (-5,5),\n",
    ")\n",
    "qmat = QMatrix(LIMITS, QSHAPE, env.action_space.n)\n",
    "\n",
    "train_matrix(qmat, env, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training (or if we choose to use a pre-trained model) we can load it from file and run it in the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from QMatrix import QMatrix\n",
    "from utils import visualize_model\n",
    "\n",
    "INNAME=\"qmatrix_models/QMatrix.pickle\"\n",
    "\n",
    "qmat = QMatrix.load(INNAME)\n",
    "visualize_model(qmat, notebook=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the Q-matrix method is entirely sufficient for tasks with small state and action spaces like this, but in practice most problems will entail impossibly large matrices. That's why outside of toy examples there is very little use for this method, and more sophisticated approaches are required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage Actor-Critic Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage actor-critic method is a newer approach to reinforcement learning which utilizes a combination of value estimation and direct policy optimization.\n",
    "\n",
    "In our case, a single neural network $\\pi$ with parameters $\\theta$ learns to predict an estimate of the value at the current state (similar to the value in the Q-matrix) as well as a set of action probabilities; more formally, \n",
    "\n",
    "$$ \\pi_{\\theta}(s_t) = \\left( V_{\\psi}(s_t), P_{\\phi}(A) \\right): \\psi, \\phi \\subset \\theta $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The part of the model's output responsible for predicting the value is called the **critic** while the one giving us the action probabilities is called the **actor**. Just like before, during each training episode a sequence of states is recorded and the sum of future rewards is computed for each. This then gets used in the critic loss\n",
    "\n",
    "$$ R_{observed}(s_t) = \\sum_{\\tau=t}^{t_{\\max}}\\gamma^{\\tau-t} R(s_{\\tau}) $$\n",
    "\n",
    "$$ \\mathcal{L}_{critic}\\left( V_{\\psi}(s_t), R_{observed}(s_t) \\right) =\n",
    " \\mathcal{L}_{crossentropy}\\left( V_{\\psi}(s_t),  R_{observed}(s_t) \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, the critic learns to associate every state with a discounted future reward -- the value function part of this approach. The agent gets optimized using the actual reward $R_{observed}(s_t)$ and the critic's estimate of the value $V_{\\psi}(s_t)$ to produce a term called **advantage** $ Adv(s_t) = R_{observed}(s_t) - V_{\\psi}(s_t)$. The loss is then defined as \n",
    "\n",
    "$$ \\mathcal{L}_{actor} = -\\sum_{t=1}^{t_{\\max}} Adv(s_t) \\log\\left[P_{\\phi}(A \\vert s_t) \\right]  $$\n",
    "\n",
    "The logarithmic probabilities $\\log \\left[P_{\\phi}(A \\vert s_t) \\right] $ of the model  are flipped from negative to positive. Thus high probabilities are close to zero while low ones tend towards infinity). \n",
    " The upshot of this is that\n",
    "\n",
    "- when the model does much better than expected, adjusting the probability upward is given greater weight;\n",
    "- when the model does much worse than expected, adjusting the probability downward is given greater weight;\n",
    "- outputs which trend towards the expected value of the state are left alone as their weights approach 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation code for this model can be found in *ActorCritic.py*. Specifically, we're using PyTorch to implement a deep neural network with one hidden layer, 128 hidden features and ReLU activation by default. This then feeds into a 2-feature action output (corresponding to *move left* and *move right*), and a single feature value output. The code for interfacing with the simulation environment is contained in the class itself, which is quite verbose and won't be considered in this notebook. Training can be invoked by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from ActorCritic import ActorCriticModel, SEED, ENV\n",
    "\n",
    "OUTNAME=\"actor_critic_models/ActorCritic_adam\"\n",
    "\n",
    "def train_model(outname=OUTNAME):\n",
    "\n",
    "    env = gym.make(ENV)\n",
    "    env.reset(seed=SEED)\n",
    "\n",
    "    dev = torch.device('cpu')\n",
    "    model = ActorCriticModel(\n",
    "        env.observation_space.shape[0],\n",
    "        env.action_space.n,\n",
    "        device=dev\n",
    "    )\n",
    "    model.train()\n",
    "\n",
    "    opt = Adam(model.parameters(), lr=3e-2)\n",
    "\n",
    "    for ep in range(1000):\n",
    "        model.train_episode(env, opt, ep)\n",
    "        if ep % 100 == 0:\n",
    "            print(f\"Saving model at epoch {ep}...\")\n",
    "            torch.save(model.state_dict(), f\"{outname}_ep{ep}.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while previously trained models can be loaded and visualized with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import visualize_model\n",
    "\n",
    "INNAME=\"actor_critic_models/ActorCritic_adam_ep600.pth\"\n",
    "\n",
    "def evaluate_model(inname=INNAME):\n",
    "\n",
    "    dev = torch.device('cpu')\n",
    "    model = ActorCriticModel(device=dev)\n",
    "    model.eval()\n",
    "    model.load_state_dict(torch.load(inname))\n",
    "    \n",
    "    visualize_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imitation learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f2a7023551a37a849cc6a70933cf64f7d1311b89ef103a770f61e685c6cfb90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
